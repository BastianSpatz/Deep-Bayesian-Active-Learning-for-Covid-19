{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's start with a bunch of imports.\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends\n",
    "import torch.utils.data as torchdata\n",
    "from torch import optim\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from baal.active import get_heuristic, ActiveLearningDataset\n",
    "from baal.active.active_loop import ActiveLearningLoop\n",
    "from baal.bayesian.dropout import patch_module\n",
    "from baal.modelwrapper import ModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    epoch: int = 5\n",
    "    batch_size: int = 2\n",
    "    initial_pool: int = 10\n",
    "    query_size: int = 1\n",
    "    lr: float = 9e-4\n",
    "    heuristic: str = 'bald'\n",
    "    iterations: int = 5\n",
    "    training_duration: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.dataset import Dataset, collate_fn_padd\n",
    "from src.dataset.utils import train_test_validation_split\n",
    "\n",
    "def get_datasets(initial_pool):\n",
    "    dataset = Dataset(path_to_npy_data=\"data/NPY/volumes/\", path_to_npy_targets=\"data/NPY/labels/\")\n",
    "\n",
    "    train, test, valid = train_test_validation_split(dataset=dataset)\n",
    "    # In a real application, you will want a validation set here.\n",
    "\n",
    "    # Here we set `pool_specifics`, where we set the transform attribute for the pool.\n",
    "    active_set = ActiveLearningDataset(train)\n",
    "\n",
    "    # We start labeling randomly.\n",
    "    active_set.label_randomly(initial_pool)\n",
    "    return active_set, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.conv_net import ConvNN\n",
    "\n",
    "hyperparams = ExperimentConfig()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "if not use_cuda:\n",
    "    print(\"warning, the experiments would take ages to run on cpu\")\n",
    "\n",
    "# Get datasets\n",
    "active_set, test_set = get_datasets(hyperparams.initial_pool)\n",
    "\n",
    "# Get our model.\n",
    "heuristic = get_heuristic(hyperparams.heuristic)\n",
    "criterion = CrossEntropyLoss()\n",
    "model = ConvNN()\n",
    "\n",
    "# change dropout layer to MCDropout\n",
    "model = patch_module(model)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyperparams.lr, momentum=0.9)\n",
    "\n",
    "# Wraps the model into a usable API.\n",
    "model = ModelWrapper(model, criterion, replicate_in_memory=False)\n",
    "\n",
    "# for ActiveLearningLoop we use a smaller batchsize\n",
    "# since we will stack predictions to perform MCDropout.\n",
    "active_loop = ActiveLearningLoop(active_set,\n",
    "                                 model.predict_on_dataset,\n",
    "                                 heuristic,\n",
    "                                 hyperparams.query_size,\n",
    "                                 batch_size=1,\n",
    "                                 iterations=hyperparams.iterations,\n",
    "                                 use_cuda=use_cuda,\n",
    "                                 verbose=False,\n",
    "                                 collate_fn=collate_fn_padd)\n",
    "\n",
    "# We will reset the weights at each active learning step so we make a copy.\n",
    "init_weights = deepcopy(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labelling_progress = active_set._labelled.copy().astype(np.uint16)\n",
    "for epoch in tqdm(range(hyperparams.epoch)):\n",
    "    # Load the initial weights.\n",
    "    model.load_state_dict(init_weights)\n",
    "\n",
    "    # Train the model on the currently labelled dataset.\n",
    "    _ = model.train_on_dataset(active_set, optimizer=optimizer, batch_size=hyperparams.batch_size,\n",
    "                               use_cuda=use_cuda, epoch=hyperparams.training_duration, collate_fn=collate_fn_padd)\n",
    "\n",
    "    # Get test NLL!\n",
    "    model.test_on_dataset(test_set, hyperparams.batch_size, use_cuda,\n",
    "                          average_predictions=hyperparams.iterations, collate_fn=collate_fn_padd)\n",
    "    metrics = model.metrics\n",
    "\n",
    "    # We can now label the most uncertain samples according to our heuristic.\n",
    "    should_continue = active_loop.step()\n",
    "    # Keep track of progress\n",
    "    labelling_progress += active_set._labelled.astype(np.uint16)\n",
    "    if not should_continue:\n",
    "        break\n",
    "\n",
    "    test_loss = metrics['test_loss'].value\n",
    "    logs = {\n",
    "        \"test_nll\": test_loss,\n",
    "        \"epoch\": epoch,\n",
    "        \"Next Training set size\": len(active_set)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight = model.state_dict()\n",
    "dataset = active_set.state_dict()\n",
    "torch.save({'model': model_weight, 'dataset': dataset, 'labelling_progress': labelling_progress},\n",
    "           'checkpoint.pth')\n",
    "print(model.state_dict().keys(), dataset.keys(), labelling_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight = torch.load(\"checkpoint.pth\")[\"model\"]\n",
    "active_set = torch.load(\"checkpoint.pth\")[\"dataset\"]\n",
    "labelling_progress = torch.load(\"checkpoint.pth\")[\"labelling_progress\"]\n",
    "print(len(active_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNN()\n",
    "model.load_state_dict(torch.load(\"checkpoint.pth\")[\"model\"])\n",
    "model.cuda()\n",
    "\n",
    "# modify our model to get features\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Make a feature extractor from our trained model.\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(self.model(x), 1)\n",
    "\n",
    "\n",
    "features = FeatureExtractor(model)\n",
    "acc = []\n",
    "for x, y in DataLoader(active_set._dataset, batch_size=2, collate_fn=collate_fn_padd):\n",
    "    acc.append((features(x.cuda()).detach().cpu().numpy(), y.detach().cpu().numpy()))\n",
    "\n",
    "xs, ys = zip(*acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Compute t-SNE on the extracted features.\n",
    "tsne = TSNE(n_jobs=4)\n",
    "transformed = tsne.fit_transform(np.vstack(xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels = np.concatenate(ys)\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal.utils.plot_utils import make_animation_from_data\n",
    "\n",
    "# Create frames to animate the process.\n",
    "frames = make_animation_from_data(transformed, labels, labelling_progress, [\"CP\", \"NCP\", \"Normal\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "def plot_images(img_list):\n",
    "    def init():\n",
    "        img.set_data(img_list[0])\n",
    "        return (img,)\n",
    "\n",
    "    def animate(i):\n",
    "        img.set_data(img_list[i])\n",
    "        return (img,)\n",
    "\n",
    "    fig = plt.Figure(figsize=(10, 10))\n",
    "    ax = fig.gca()\n",
    "    img = ax.imshow(img_list[0])\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                   frames=len(img_list), interval=60, blit=True)\n",
    "    return anim\n",
    "\n",
    "\n",
    "HTML(plot_images(frames).to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ba41da815aa047041a42bb235514dd275a168b3f2f58893abf50244d8fb0ae4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
