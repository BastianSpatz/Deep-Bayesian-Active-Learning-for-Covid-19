{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from src.models.conv_net import MnistExampleModel\n",
    "from src.dataset.dataset import CustomDataset, collate_fn_padd, CustomFileDataset\n",
    "from src.dataset.utils import get_active_learning_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/Bastian/Documents/Master Mathematik/MasterArbeit/Deep Bayesian Active Learning for Covid-19 Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistExampleModel()\n",
    "dataset = CustomDataset(data_path= data_path +\"/NPY/volumes/\", \n",
    "                                      target_path=data_path + \"/NPY/labels/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP sample len 1531\n",
      "NCP sample len 1473\n",
      "Normal sample len 1078\n",
      "Number of samples per class: 862\n",
      "size train files: 2586\n",
      "size test files: 645\n",
      "size initial pool files: 3\n"
     ]
    }
   ],
   "source": [
    "train_indices, test_indices, initial_pool = get_active_learning_datasets(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal.active import ActiveLearningDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "# We use -1 to specify that the data is unlabeled.\n",
    "train_dataset = CustomFileDataset([dataset.data_paths[idx] for idx in train_indices],\n",
    "                                  [dataset.targets[idx] for idx in train_indices],\n",
    "                                    train_transform)\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "# We use -1 to specify that the data is unlabeled.\n",
    "test_dataset = CustomFileDataset([dataset.data_paths[idx] for idx in test_indices],\n",
    "                                  [dataset.targets[idx] for idx in test_indices],\n",
    "                                    test_transform)\n",
    "active_learning_ds = ActiveLearningDataset(train_dataset, pool_specifics={'transform': test_transform})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal.active import ActiveLearningDataset\n",
    "\n",
    "train_indices, test_indices, initial_pool = dataset.get_active_learning_datasets(3)\n",
    "train = torch.utils.data.Subset(dataset, train_indices)\n",
    "test = torch.utils.data.Subset(dataset, test_indices)\n",
    "print(\"train set length {}\".format(len(train)))\n",
    "print(\"test set length {}\".format(len(test)))\n",
    "# Here we set `pool_specifics`, where we set the transform attribute for the pool.\n",
    "active_set = ActiveLearningDataset(train)\n",
    "active_set.label(initial_pool)\n",
    "print(f\"Num. labeled: {len(active_set)}/{len(train_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from baal.modelwrapper import ModelWrapper\n",
    "from baal.bayesian.dropout import MCDropoutModule\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# This will modify all Dropout layers to be usable at test time which is\n",
    "# required to perform Active Learning.\n",
    "model = MCDropoutModule(model)\n",
    "if USE_CUDA:\n",
    "  model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# ModelWrapper is an object similar to keras.Model.\n",
    "baal_model = ModelWrapper(model, criterion, replicate_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal.active.heuristics import BALD\n",
    "heuristic = BALD(shuffle_prop=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[811, 422, 672]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. labeled: 3/2586\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Let's label 100 training examples randomly first.\n",
    "# Note: the indices here are relative to the pool of unlabelled items!\n",
    "active_learning_ds.can_label = False\n",
    "# train_idxs = np.random.permutation(np.arange(len(train_dataset)))[:9].tolist()\n",
    "active_learning_ds.label(initial_pool)\n",
    "\n",
    "print(f\"Num. labeled: {len(active_learning_ds)}/{len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6064-MainThread ] [baal.modelwrapper:train_on_dataset:83] 2023-02-06T16:16:56.223684Z [info     ] Starting training              dataset=3 epoch=1\n",
      "[6064-MainThread ] [baal.modelwrapper:train_on_dataset:94] 2023-02-06T16:17:04.928177Z [info     ] Training complete              train_loss=0.837359607219696\n",
      "[6064-MainThread ] [baal.modelwrapper:test_on_dataset:123] 2023-02-06T16:17:04.937728Z [info     ] Starting evaluating            dataset=645\n",
      "[6064-MainThread ] [baal.modelwrapper:test_on_dataset:133] 2023-02-06T16:19:15.199137Z [info     ] Evaluation complete            test_loss=1.098615288734436\n",
      "Metrics: {'test_loss': 1.098615288734436, 'train_loss': 0.837359607219696}\n"
     ]
    }
   ],
   "source": [
    "# 2. Train the model for a few epoch on the training set.\n",
    "baal_model.train_on_dataset(active_learning_ds, optimizer, batch_size=1, epoch=1, collate_fn=collate_fn_padd, workers=4, use_cuda=USE_CUDA)\n",
    "baal_model.test_on_dataset(test_dataset, batch_size=1, collate_fn=collate_fn_padd, workers=4, use_cuda=USE_CUDA)\n",
    "\n",
    "print(\"Metrics:\", {k:v.avg for k,v in baal_model.metrics.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "idx = random.sample(range(len(active_learning_ds.pool)), 100)\n",
    "active_learning_ds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6064-MainThread ] [baal.modelwrapper:predict_on_dataset_generator:232] 2023-02-06T16:19:15.290361Z [info     ] Start Predict                  dataset=2583\n",
      "100%|██████████| 2583/2583 [08:01<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 3. Select the K-top uncertain samples according to the heuristic.\n",
    "pool = active_learning_ds.pool\n",
    "if len(pool) == 0:\n",
    "  raise ValueError(\"We're done!\")\n",
    "# We make 15 MCDropout iterations to approximate the uncertainty.\n",
    "predictions = baal_model.predict_on_dataset(pool, batch_size=1, collate_fn=collate_fn_padd, iterations=1, use_cuda=USE_CUDA, verbose=True)\n",
    "# We will label the 10 most uncertain samples.\n",
    "top_uncertainty = heuristic(predictions)[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for idx in top_uncertainty:\n",
    "    print(pool[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Label those samples.\n",
    "oracle_indices = active_learning_ds._pool_to_oracle_index(top_uncertainty)\n",
    "# labels = [get_label(train_dataset.files[idx]) for idx in oracle_indices]\n",
    "# print(list(zip(labels, oracle_indices)))\n",
    "active_learning_ds.label(top_uncertainty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ba41da815aa047041a42bb235514dd275a168b3f2f58893abf50244d8fb0ae4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
