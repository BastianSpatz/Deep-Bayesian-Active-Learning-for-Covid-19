{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "from torch import optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from baal.active import ActiveLearningDataset, get_heuristic\n",
    "from baal.bayesian.dropout import patch_module\n",
    "from baal.utils.pytorch_lightning import (\n",
    "    ActiveLightningModule,\n",
    "    ResetCallback,\n",
    "    BaalTrainer,\n",
    "    BaaLDataModule,\n",
    ")\n",
    "from src.dataset.dataset import Dataset\n",
    "from src.dataset.utils import train_test_validation_split\n",
    "from torchvision.models import vgg16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10DataModule(BaaLDataModule):\n",
    "    def __init__(self):\n",
    "        train_transform = transforms.Compose(\n",
    "            [transforms.Resize((256, 256))]\n",
    "        )\n",
    "        test_transform = transforms.Compose([transforms.Resize((256, 256))])\n",
    "        dataset = Dataset(path_to_npy_data=\"data/NPY/volumes/\",\n",
    "                             path_to_npy_targets=\"data/NPY/labels/\")\n",
    "        train_ds, self.test_set, valid_ds = train_test_validation_split(dataset)\n",
    "        self.active_set = ActiveLearningDataset(\n",
    "            train_ds,\n",
    "        )\n",
    "        super().__init__(\n",
    "            active_dataset=self.active_set,\n",
    "            batch_size=2,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        return DataLoader(self.active_dataset, self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        return DataLoader(self.test_set, self.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(ActiveLightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.name = \"VGG16\"\n",
    "        self.version = \"0.0.1\"\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # We use `patch_module` to swap Dropout modules in the model\n",
    "        # for our implementation which enables MC-Dropou\n",
    "        self.vgg16 = patch_module(vgg16(num_classes=3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss_val, prog_bar=True, on_epoch=True)\n",
    "        return loss_val\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss_val, prog_bar=True, on_epoch=True)\n",
    "        return loss_val\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        return whatever optimizers we want here\n",
    "        :return: list of optimizers\n",
    "        \"\"\"\n",
    "        optimizer = optim.SGD(\n",
    "            self.parameters(), lr=1e-4, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        return [optimizer], []\n",
    "\n",
    "    @classmethod\n",
    "    def add_model_specific_args(cls, parser):\n",
    "        parser.add_argument(\"--num_classes\", type=int, default=3)\n",
    "        parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n",
    "        parser.add_argument(\n",
    "            \"--iterations\", type=int, default=1, help=\"Number of MC-Sampling to perform\"\n",
    "        )\n",
    "        parser.add_argument(\"--replicate_in_memory\", type=bool, default=False)\n",
    "        parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "def parse_arguments():\n",
    "    parser = ArgumentParser()\n",
    "    parser = VGG16.add_model_specific_args(parser)\n",
    "    parser = ArgumentParser(parents=[parser], conflict_handler=\"resolve\", add_help=False)\n",
    "    parser.add_argument(\"--heuristic\", type=str, default=\"bald\", help=\"Which heuristic to use.\")\n",
    "    parser.add_argument(\"--data_root\", type=str, default=\"/tmp\", help=\"Where to store data.\")\n",
    "    parser.add_argument(\n",
    "        \"--query_size\", type=int, default=20, help=\"How many items to label per step.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--training_duration\", type=int, default=1, help=\"How many epochs per step.\"\n",
    "    )\n",
    "    parser.add_argument(\"--gpus\", type=int, default=1, help=\"How many GPUs to use.\")\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def main():\n",
    "    pl.seed_everything(42)\n",
    "    # Create our dataset.\n",
    "    # args = parse_arguments()\n",
    "    datamodule = Cifar10DataModule()\n",
    "    datamodule.active_dataset.label_randomly(100)\n",
    "    # Get our heuristic to compute uncertainty.\n",
    "    heuristic = get_heuristic(\"bald\", shuffle_prop=0.0, reduction=\"none\")\n",
    "    model = VGG16()  # Instantiate VGG16\n",
    "\n",
    "    # Make our PL Trainer\n",
    "    trainer = BaalTrainer(\n",
    "        # The weights of the model will change as it gets\n",
    "        # trained; we need to keep a copy (deepcopy) so that\n",
    "        # we can reset them.\n",
    "        callbacks=[ResetCallback(copy.deepcopy(model.state_dict()))],\n",
    "        dataset=datamodule.active_dataset,\n",
    "        max_epochs=1,\n",
    "        heuristic=heuristic,\n",
    "        query_size=20,\n",
    "    )\n",
    "\n",
    "    AL_STEPS = 100\n",
    "    for al_step in range(AL_STEPS):\n",
    "        print(f\"Step {al_step} Dataset size {len(datamodule.active_dataset)}\")\n",
    "        trainer.fit(model, datamodule=datamodule)  # Train the model on the labelled set.\n",
    "        trainer.test(model, datamodule=datamodule)  # Get test performance.\n",
    "        should_continue = trainer.step(\n",
    "            model, datamodule=datamodule\n",
    "        )  # Label the top-k most uncertain examples.\n",
    "        if not should_continue:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bastian\\Documents\\Master Mathematik\\MasterArbeit\\Deep Bayesian Active Learning for Covid-19 Diagnosis\\Deep-Bayesian-Active-Learning-for-Covid-19\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\baal\\active\\dataset\\pytorch_dataset.py:226: UserWarning: \n",
      "It seems that data augmentation is not disabled when iterating on the pool.\n",
      "You can disable it by overriding attributes using `pool_specifics` \n",
      "when instantiating ActiveLearningDataset.\n",
      "Example:\n",
      "```\n",
      "from torchvision.transforms import *\n",
      "train_transform = Compose([Resize((224, 224)), RandomHorizontalFlip(),\n",
      "                            RandomRotation(30), ToTensor()])\n",
      "test_transform = Compose([Resize((224, 224)),ToTensor()])\n",
      "dataset = CIFAR10(..., transform=train_transform)\n",
      "\n",
      "al_dataset = ActiveLearningDataset(dataset,\n",
      "                                    pool_specifics={'transform': test_transform})\n",
      "```   \n",
      "\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1764: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Dataset size 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:294: LightningDeprecationWarning: `LightningDataModule.on_save_checkpoint` was deprecated in v1.6 and will be removed in v1.8. Use `state_dict` instead.\n",
      "  rank_zero_deprecation(\n",
      "c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:299: LightningDeprecationWarning: `LightningDataModule.on_load_checkpoint` was deprecated in v1.6 and will be removed in v1.8. Use `load_state_dict` instead.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | vgg16     | VGG              | 134 M \n",
      "-----------------------------------------------\n",
      "134 M     Trainable params\n",
      "0         Non-trainable params\n",
      "134 M     Total params\n",
      "537.091   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534aeedd414c43e9a5ba82a82b890903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 137, in default_collate\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Bastian\\Documents\\Master Mathematik\\MasterArbeit\\Deep Bayesian Active Learning for Covid-19 Diagnosis\\Deep-Bayesian-Active-Learning-for-Covid-19\\notebooks\\active_learning_testing.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32mc:\\Users\\Bastian\\Documents\\Master Mathematik\\MasterArbeit\\Deep Bayesian Active Learning for Covid-19 Diagnosis\\Deep-Bayesian-Active-Learning-for-Covid-19\\notebooks\\active_learning_testing.ipynb Cell 7\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m al_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(AL_STEPS):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStep \u001b[39m\u001b[39m{\u001b[39;00mal_step\u001b[39m}\u001b[39;00m\u001b[39m Dataset size \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(datamodule\u001b[39m.\u001b[39mactive_dataset)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdatamodule)  \u001b[39m# Train the model on the labelled set.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     trainer\u001b[39m.\u001b[39mtest(model, datamodule\u001b[39m=\u001b[39mdatamodule)  \u001b[39m# Get test performance.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     should_continue \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mstep(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         model, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bastian/Documents/Master%20Mathematik/MasterArbeit/Deep%20Bayesian%20Active%20Learning%20for%20Covid-19%20Diagnosis/Deep-Bayesian-Active-Learning-for-Covid-19/notebooks/active_learning_testing.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     )  \u001b[39m# Label the top-k most uncertain examples.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    698\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    731\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    733\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    734\u001b[0m )\n\u001b[1;32m--> 735\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    737\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1283\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1282\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:271\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[0;32m    268\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    269\u001b[0m )\n\u001b[0;32m    270\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:174\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[0;32m    173\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 174\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[0;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:263\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    261\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[0;32m    264\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[0;32m    265\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:277\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_next_batch\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m--> 277\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfetched \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_len:\n\u001b[0;32m    280\u001b[0m         \u001b[39m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:557\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \n\u001b[0;32m    554\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[39m        a collections of batch data\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py:569\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[1;34m(loader_iters)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    560\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    561\u001b[0m     \u001b[39m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \n\u001b[0;32m    563\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39m        Any: a collections of batch data\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:99\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[0;32m    103\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1223\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1250\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1251\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 457\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 172, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"c:\\Users\\Bastian\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 137, in default_collate\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataModule(ImageClassificationData):\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return 3\n",
    "\n",
    "\n",
    "def get_data_module(heuristic):\n",
    "    dataset = Dataset(path_to_npy_data=\"data/NPY/volumes/\", path_to_npy_targets=\"data/NPY/labels/\")\n",
    "    train_set, test_set, valid = train_test_validation_split(dataset=dataset)\n",
    "    dm = ImageDataModule.from_datasets(\n",
    "        train_dataset=train_set,\n",
    "        test_dataset=test_set,\n",
    "        transform_kwargs=dict(image_size=(256, 256)),\n",
    "        batch_size=2,\n",
    "    )\n",
    "    active_dm = ActiveLearningDataModule(\n",
    "        dm,\n",
    "        heuristic=get_heuristic(heuristic),\n",
    "        initial_num_labels=1024,\n",
    "        query_size=100,\n",
    "        val_split=0.0,\n",
    "    )\n",
    "    assert active_dm.has_test, \"No test set?\"\n",
    "    return active_dm\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(512, 512),\n",
    "        nn.ReLU(True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(512, 3),\n",
    "    )\n",
    "    LR = 0.001\n",
    "    model = ImageClassifier(\n",
    "        num_classes=3,\n",
    "        head=head,\n",
    "        backbone=\"vgg16\",\n",
    "        pretrained=True,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=partial(torch.optim.SGD, momentum=0.9, weight_decay=5e-4),\n",
    "        learning_rate=LR,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = 1 if torch.cuda.is_available() else 0\n",
    "active_dm: ActiveLearningDataModule = get_data_module(\"bald\")\n",
    "model: ImageClassifier = get_model()\n",
    "# We use Flash trainer without validation set.\n",
    "# In practice, using a validation set is risky because we overfit often.\n",
    "trainer = flash.Trainer(\n",
    "    gpus=gpus,\n",
    "    max_epochs=2500,\n",
    "    limit_val_batches=0,\n",
    ")\n",
    "\n",
    "# We will train for 20 epochs before doing 20 MC-Dropout iterations to estimate uncertainty.\n",
    "active_learning_loop = ActiveLearningLoop(label_epoch_frequency=20, inference_iteration=20)\n",
    "active_learning_loop.connect(trainer.fit_loop)\n",
    "trainer.fit_loop = active_learning_loop\n",
    "# We do not freeze the backbone, this gives better performance.\n",
    "trainer.finetune(model, datamodule=active_dm, strategy=\"no_freeze\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ba41da815aa047041a42bb235514dd275a168b3f2f58893abf50244d8fb0ae4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
